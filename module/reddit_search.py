# í•µì‹¬ ê¸°ëŠ¥:
#   ì§€ì •ëœ ì„œë¸Œë ˆë”§ì—ì„œ ì£¼ì–´ì§„ í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²Œì‹œê¸€ì„ ê²€ìƒ‰í•˜ê³ ,
#   ìƒˆë¡œìš´ ê²Œì‹œê¸€ì„ CSV íŒŒì¼ì— ì—…ë°ì´íŠ¸í•œ í›„ ë¡œê·¸ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.
#
# ì£¼ìš” íŒŒë¼ë¯¸í„°:
#   get_new_posts(keyword, subreddit, limit): ì„œë¸Œë ˆë”§ì—ì„œ ê²Œì‹œê¸€ì„ ê²€ìƒ‰ í›„ DataFrame ë°˜í™˜.
#   update(search_subreddit, keyword, csv_filename_base): CSV ì—…ë°ì´íŠ¸ ë° ë¡œê·¸ ê¸°ë¡.

import os
import sys
import time
import pandas as pd
import praw
from datetime import datetime, timezone
from dotenv import load_dotenv

try:
    import streamlit as st
    if hasattr(st, "secrets") and "REDDIT_CLIENT_ID" in st.secrets:
        secrets = st.secrets
        IS_STREAMLIT = True
    else:
        raise ImportError("Streamlit secrets not configured properly.")
except:
    BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    load_dotenv(os.path.join(BASE_DIR, ".env"))
    secrets = os.environ

BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
DATA_DIR = os.path.join(BASE_DIR, "data")
LOG_PATH = os.path.join(BASE_DIR, "run_log.txt")
LIMIT = 500

reddit = praw.Reddit(
    client_id=secrets["REDDIT_CLIENT_ID"],
    client_secret=secrets["REDDIT_CLIENT_SECRET"],
    username=secrets["USERNAME"],
    password=secrets["PASSWORD"],
    user_agent=secrets["REDDIT_USER_AGENT"]
)

def load_csv(path):
    """
    ì£¼ì–´ì§„ ê²½ë¡œì˜ CSV íŒŒì¼ì„ DataFrameìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.
    (ì‹œê°„ ì»¬ëŸ¼ì€ datetimeìœ¼ë¡œ íŒŒì‹±)
    """
    if os.path.exists(path):
        return pd.read_csv(path, parse_dates=["time"])
    return pd.DataFrame()

def get_new_posts(keyword, subreddit="all", limit=LIMIT):
    """
    ì§€ì •ëœ subreddit(ê¸°ë³¸: "all")ì—ì„œ, ìž…ë ¥ë°›ì€ keywordë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìµœì‹  ê²Œì‹œê¸€ì„ ê²€ìƒ‰í•˜ì—¬
    DataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    results = []
    try:
        for post in reddit.subreddit(subreddit).search(keyword, sort="new", limit=limit):
            post_time = datetime.fromtimestamp(post.created_utc, tz=timezone.utc)
            results.append({
                "id": post.id,
                "subreddit": post.subreddit.display_name,
                "title": post.title,
                "selftext": post.selftext,
                "time": post_time,
                "score": post.score,
                "num_comments": post.num_comments
            })
        time.sleep(2)
    except Exception as e:
        print(f"ðŸ”´ Error during fetching: {e}")
    return pd.DataFrame(results)

def update(search_subreddit="all", keyword="hoka", csv_filename_base=None):
    """
    [ë§¤ê°œë³€ìˆ˜]
      - search_subreddit: ê²€ìƒ‰ ëŒ€ìƒ ì„œë¸Œë ˆë”§ (ê¸°ë³¸: "all")
      - keyword: ê²€ìƒ‰ì–´ (ê¸°ë³¸: "hoka")
      - csv_filename_base: CSV íŒŒì¼ëª… ê¸°ë³¸ì´ë¦„; ë¯¸ì§€ì • ì‹œ keyword ê¸°ë°˜ íŒŒì¼("hoka_posts.csv") ì‚¬ìš©,
                            ì§€ì • ì‹œ data/{csv_filename_base}_posts.csv í˜•ì‹ìœ¼ë¡œ ì €ìž¥.

    CSV íŒŒì¼ì— ìƒˆë¡œìš´ í¬ìŠ¤íŠ¸ë¥¼ ì¶”ê°€í•˜ê³ , ìž‘ì—… ê²°ê³¼ë¥¼ ë¡œê·¸ì— ê¸°ë¡í•©ë‹ˆë‹¤.
    """

    if csv_filename_base is None:
        csv_path = os.path.join(DATA_DIR, f"{keyword.lower().replace(' ', '_')}_posts.csv")
        display_name = keyword.lower().replace(" ", "_")
    else:
        csv_path = os.path.join(DATA_DIR, f"{csv_filename_base.lower()}_posts.csv")
        display_name = csv_filename_base.lower()

    os.makedirs(os.path.dirname(csv_path), exist_ok=True)

    old = load_csv(csv_path)
    new = get_new_posts(keyword, subreddit=search_subreddit)

    if not old.empty:
        before = len(new)
        new = new[~new['id'].isin(old['id'])]
        duplicates = before - len(new)
    else:
        duplicates = 0

    if not new.empty:
        combined = pd.concat([old, new], ignore_index=True)
        combined = combined.sort_values("time", ascending=False)
        combined.to_csv(csv_path, index=False, encoding="utf-8-sig")
        msg = f"[{datetime.now()}] [Subreddit: {search_subreddit}] [{display_name}] {len(new)}ê°œ ì €ìž¥ / ì¤‘ë³µ {duplicates}ê°œ ì œê±°\n"
    else:
        if not old.empty:
            old = old.sort_values("time", ascending=False)
            old.to_csv(csv_path, index=False, encoding="utf-8-sig")
        msg = f"[{datetime.now()}] [Subreddit: {search_subreddit}] [{display_name}] ìƒˆë¡œìš´ í¬ìŠ¤íŠ¸ ì—†ìŒ / ì¤‘ë³µ {duplicates}ê°œ\n"

    with open(LOG_PATH, "a", encoding="utf-8") as f:
        f.write(msg)

    print(msg.strip())

# CLI ì‹¤í–‰ìš© ì˜ˆì‹œ:
#   ê¸°ë³¸ ì‹¤í–‰ (ê¸°ë³¸ ì„œë¸Œë ˆë”§ "all", ê¸°ë³¸ ê²€ìƒ‰ì–´ "hoka"):
#       python reddit_search.py
#   ì„œë¸Œë ˆë”§, ê²€ìƒ‰ì–´, ê·¸ë¦¬ê³  CSV íŒŒì¼ ê¸°ë³¸ì´ë¦„ ì§€ì •:
#       python reddit_search.py sports baseball sports_posts

if __name__ == "__main__":
    # CLI ì¸ìˆ˜:
    #   1ë²ˆì§¸ ì¸ìˆ˜ (optional): ê²€ìƒ‰ ëŒ€ìƒ subreddit (ê¸°ë³¸: "all")
    #   2ë²ˆì§¸ ì¸ìˆ˜ (optional): ê²€ìƒ‰ ëŒ€ìƒ keyword (ê¸°ë³¸: "hoka")
    #   3ë²ˆì§¸ ì¸ìˆ˜ (optional): CSV íŒŒì¼ëª… ê¸°ë³¸ì´ë¦„ (ë¯¸ì§€ì • ì‹œ keyword ê¸°ë°˜ íŒŒì¼ ì‚¬ìš©)
    args = sys.argv[1:]
    search_subreddit = args[0] if len(args) > 0 else "all"
    keyword = args[1] if len(args) > 1 else "hoka"
    csv_filename_base = args[2] if len(args) > 2 else None
    update(search_subreddit=search_subreddit, keyword=keyword, csv_filename_base=csv_filename_base)