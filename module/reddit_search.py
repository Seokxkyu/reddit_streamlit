# import os
# import sys
# import time
# from dotenv import load_dotenv
# import praw
# from datetime import datetime, timezone
# import pandas as pd

# load_dotenv()

# reddit = praw.Reddit(
#     client_id=os.getenv("REDDIT_CLIENT_ID"),
#     client_secret=os.getenv("REDDIT_CLIENT_SECRET"),
#     password=os.getenv("REDDIT_PASSWORD"),
#     username=os.getenv("REDDIT_USERNAME"),
#     user_agent=os.getenv("REDDIT_USER_AGENT")
# )

# BASE_DIR = os.path.dirname(os.path.abspath(__file__))
# LOG_PATH = os.path.join(BASE_DIR, 'run_log.txt')
# LIMIT = 500

# def get_new_posts(keyword, limit=LIMIT):
#     results = []

#     try:
#         for post in reddit.subreddit("all").search(keyword, sort="new", limit=limit):
#             post_time = datetime.fromtimestamp(post.created_utc, tz=timezone.utc)
#             results.append({
#                 "id": post.id,
#                 "subreddit": post.subreddit.display_name,
#                 "title": post.title,
#                 "selftext": post.selftext,
#                 "time": post_time,
#                 "score": post.score,
#                 "num_comments": post.num_comments
#             })
#         time.sleep(2)
#     except Exception as e:
#         print(f"ğŸ”´ Error during fetching: {e}")

#     return pd.DataFrame(results)

# def load_csv(path):
#     if os.path.exists(path):
#         return pd.read_csv(path, parse_dates=["time"])
#     return pd.DataFrame()

# def update(keyword):
#     keyword_safe = keyword.lower().replace(" ", "_")
#     csv_path = os.path.join(BASE_DIR, "data", f"{keyword_safe}_posts.csv")
#     os.makedirs(os.path.dirname(csv_path), exist_ok=True)

#     old = load_csv(csv_path)
#     new = get_new_posts(keyword)

#     if not old.empty:
#         before = len(new)
#         new = new[~new['id'].isin(old['id'])]
#         duplicates = before - len(new)
#     else:
#         duplicates = 0

#     if not new.empty:
#         combined = pd.concat([old, new], ignore_index=True)
#         combined = combined.sort_values("time", ascending=False)
#         combined.to_csv(csv_path, index=False)
#         msg = f"[{datetime.now()}] [{keyword}] {len(new)}ê°œ ì €ì¥ / ì¤‘ë³µ {duplicates}ê°œ ì œê±°\n"
#     else:
#         old = old.sort_values("time", ascending=False)
#         old.to_csv(csv_path, index=False)
#         msg = f"[{datetime.now()}] [{keyword}] ìƒˆë¡œìš´ í¬ìŠ¤íŠ¸ ì—†ìŒ / ì¤‘ë³µ {duplicates}ê°œ\n"

#     with open(LOG_PATH, "a", encoding="utf-8") as f:
#         f.write(msg)

#     print(msg.strip())

# # âœ… ì‹¤í–‰ ì‹œ í‚¤ì›Œë“œ ì…ë ¥ ë°›ê¸°
# if __name__ == "__main__":
#     if len(sys.argv) > 1:
#         update(sys.argv[1])
#     else:
#         print("â— í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”. ì˜ˆ: python keyword_scraper.py hoka")

import os
import sys
import time
from dotenv import load_dotenv
import praw
from datetime import datetime, timezone
import pandas as pd

# âœ… í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ ê²½ë¡œë¡œ ìˆ˜ì •
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_DIR = os.path.join(BASE_DIR, "data")
LOG_PATH = os.path.join(BASE_DIR, "run_log.txt")

# âœ… ë£¨íŠ¸ì—ì„œ .env ëª…ì‹œì ìœ¼ë¡œ ë¡œë“œ
load_dotenv(os.path.join(BASE_DIR, ".env"))

reddit = praw.Reddit(
    client_id=os.getenv("REDDIT_CLIENT_ID"),
    client_secret=os.getenv("REDDIT_CLIENT_SECRET"),
    password=os.getenv("REDDIT_PASSWORD"),
    username=os.getenv("REDDIT_USERNAME"),
    user_agent=os.getenv("REDDIT_USER_AGENT")
)

LIMIT = 500

def get_new_posts(keyword, limit=LIMIT):
    results = []
    try:
        for post in reddit.subreddit("all").search(keyword, sort="new", limit=limit):
            post_time = datetime.fromtimestamp(post.created_utc, tz=timezone.utc)
            results.append({
                "id": post.id,
                "subreddit": post.subreddit.display_name,
                "title": post.title,
                "selftext": post.selftext,
                "time": post_time,
                "score": post.score,
                "num_comments": post.num_comments
            })
        time.sleep(2)
    except Exception as e:
        print(f"ğŸ”´ Error during fetching: {e}")

    return pd.DataFrame(results)

def load_csv(path):
    if os.path.exists(path):
        return pd.read_csv(path, parse_dates=["time"])
    return pd.DataFrame()

def update(keyword):
    keyword_safe = keyword.lower().replace(" ", "_")
    csv_path = os.path.join(DATA_DIR, f"{keyword_safe}_posts.csv")
    os.makedirs(os.path.dirname(csv_path), exist_ok=True)

    old = load_csv(csv_path)
    new = get_new_posts(keyword)

    if not old.empty:
        before = len(new)
        new = new[~new['id'].isin(old['id'])]
        duplicates = before - len(new)
    else:
        duplicates = 0

    if not new.empty:
        combined = pd.concat([old, new], ignore_index=True)
        combined = combined.sort_values("time", ascending=False)
        combined.to_csv(csv_path, index=False)
        msg = f"[{datetime.now()}] [{keyword}] {len(new)}ê°œ ì €ì¥ / ì¤‘ë³µ {duplicates}ê°œ ì œê±°\n"
    else:
        old = old.sort_values("time", ascending=False)
        old.to_csv(csv_path, index=False)
        msg = f"[{datetime.now()}] [{keyword}] ìƒˆë¡œìš´ í¬ìŠ¤íŠ¸ ì—†ìŒ / ì¤‘ë³µ {duplicates}ê°œ\n"

    with open(LOG_PATH, "a", encoding="utf-8") as f:
        f.write(msg)

    print(msg.strip())

# âœ… ì‹¤í–‰ ì‹œ í‚¤ì›Œë“œ ì…ë ¥ ë°›ê¸°
if __name__ == "__main__":
    if len(sys.argv) > 1:
        update(sys.argv[1])
    else:
        print("â— í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”. ì˜ˆ: python reddit_search.py hoka")
