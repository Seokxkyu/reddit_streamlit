import os
import time
from datetime import datetime, timedelta, timezone
import pandas as pd
import praw
from dotenv import load_dotenv

# âœ… ë£¨íŠ¸ ê¸°ì¤€ ê²½ë¡œ
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ: Streamlit ìš°ì„ , ì—†ìœ¼ë©´ .env ì²˜ë¦¬
try:
    import streamlit as st
    required_keys = ["REDDIT_CLIENT_ID", "REDDIT_CLIENT_SECRET", "PASSWORD", "USERNAME", "REDDIT_USER_AGENT"]
    if hasattr(st, "secrets") and all(key in st.secrets for key in required_keys):
        secrets = st.secrets
    else:
        raise ImportError("Streamlit secrets not configured properly.")
except Exception as e:
    load_dotenv(os.path.join(BASE_DIR, ".env"))
    secrets = {}
    missing = []
    for key in ["REDDIT_CLIENT_ID", "REDDIT_CLIENT_SECRET", "PASSWORD", "USERNAME", "REDDIT_USER_AGENT"]:
        value = os.environ.get(key)
        if not value:
            missing.append(key)
        secrets[key] = value
    if missing:
        raise ValueError(f"Missing environment variables: {', '.join(missing)}")

# âœ… ê²½ë¡œ ì„¤ì •
DATA_DIR = os.path.join(BASE_DIR, "data")
CSV_PATH = os.path.join(DATA_DIR, "hoka_posts.csv")  # ê¸°ë³¸ CSV íŒŒì¼ ê²½ë¡œ
LOG_PATH = os.path.join(BASE_DIR, "run_log.txt")

# ê¸°ë³¸ê°’ ì„¤ì •
KEYWORD = "hoka"
DAYS_BACK = 7
LIMIT = 250

# âœ… PRAW ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
reddit = praw.Reddit(
    client_id=secrets["REDDIT_CLIENT_ID"],
    client_secret=secrets["REDDIT_CLIENT_SECRET"],
    password=secrets["PASSWORD"],
    username=secrets["USERNAME"],
    user_agent=secrets["REDDIT_USER_AGENT"]
)

def get_new_posts(keyword=KEYWORD, days=DAYS_BACK, limit=LIMIT):
    """
    ìž…ë ¥ë°›ì€ í‚¤ì›Œë“œë¡œ redditì˜ ëª¨ë“  ì„œë¸Œë ˆë”§ì—ì„œ ìµœì‹  ê²Œì‹œê¸€ì„ ê²€ìƒ‰í•˜ì—¬ DataFrameìœ¼ë¡œ ë°˜í™˜
    """
    results = []
    cutoff = datetime.now(timezone.utc) - timedelta(days=days)
    
    try:
        for post in reddit.subreddit("all").search(keyword, sort="new", limit=limit):
            post_time = datetime.fromtimestamp(post.created_utc, tz=timezone.utc)
            if post_time >= cutoff:
                results.append({
                    "id": post.id,
                    "subreddit": post.subreddit.display_name,
                    "title": post.title,
                    "selftext": post.selftext,
                    "time": post_time,
                    "score": post.score,
                    "num_comments": post.num_comments
                })
        time.sleep(2)
    except Exception as e:
        print(f"ðŸ”´ Error during fetching: {e}")
    
    return pd.DataFrame(results)

def load_csv(path):
    """
    ì§€ì • ê²½ë¡œì˜ CSV íŒŒì¼ì„ DataFrameìœ¼ë¡œ ë¡œë“œ (ì‹œê°„ ì»¬ëŸ¼ì€ datetimeìœ¼ë¡œ íŒŒì‹±)
    """
    if os.path.exists(path):
        return pd.read_csv(path, parse_dates=["time"])
    return pd.DataFrame()

def update(csv_filename=None, keyword=KEYWORD):
    """
    CSV íŒŒì¼ëª…ê³¼ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ë°›ì•„,
    í•´ë‹¹ CSV íŒŒì¼ì— ìƒˆë¡œìš´ í¬ìŠ¤íŠ¸ë¥¼ ì—…ë°ì´íŠ¸í•˜ê³  ë¡œê·¸ì— ê¸°ë¡í•©ë‹ˆë‹¤.
    
    - csv_filename ì¸ìˆ˜ê°€ ì œê³µë˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ CSV_PATH (ì¦‰, "hoka_posts.csv")ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    - ê²€ìƒ‰ í‚¤ì›Œë“œ ê¸°ë³¸ê°’ì€ 'hoka'ìž…ë‹ˆë‹¤.
    """
    # csv_filename ì´ ì—†ìœ¼ë©´ CSV_PATH ë¥¼ ì‚¬ìš©
    if csv_filename is None:
        csv_path = CSV_PATH
        display_csv_name = os.path.basename(CSV_PATH).replace("_posts.csv", "")
    else:
        csv_path = os.path.join(DATA_DIR, f"{csv_filename.lower()}_posts.csv")
        display_csv_name = csv_filename.lower()
        
    os.makedirs(os.path.dirname(csv_path), exist_ok=True)
    
    old = load_csv(csv_path)
    new = get_new_posts(keyword)
    
    if not old.empty:
        before = len(new)
        new = new[~new['id'].isin(old['id'])]
        duplicates = before - len(new)
    else:
        duplicates = 0
    
    if not new.empty:
        combined = pd.concat([old, new], ignore_index=True)
        combined = combined.sort_values("time", ascending=False)
        combined.to_csv(csv_path, index=False)
        msg = f"[{datetime.now()}] [{display_csv_name}] - í‚¤ì›Œë“œ '{keyword}'ë¡œ {len(new)}ê°œì˜ ìƒˆë¡œìš´ í¬ìŠ¤íŠ¸ ì €ìž¥. / ì¤‘ë³µ {duplicates}ê°œ ì œê±°.\n"
    else:
        if not old.empty:
            old = old.sort_values("time", ascending=False)
            old.to_csv(csv_path, index=False)
        msg = f"[{datetime.now()}] [{display_csv_name}] - í‚¤ì›Œë“œ '{keyword}'ë¡œ ìƒˆë¡œìš´ í¬ìŠ¤íŠ¸ ì—†ìŒ. / ì¤‘ë³µ {duplicates}ê°œ.\n"
    
    with open(LOG_PATH, "a", encoding="utf-8") as f:
        f.write(msg)
    
    print(msg.strip())

if __name__ == "__main__":
    import sys
    # ì²« ë²ˆì§¸ ì¸ìˆ˜: CSV íŒŒì¼ëª…, ë‘ ë²ˆì§¸ ì¸ìˆ˜: ê²€ìƒ‰ í‚¤ì›Œë“œ.
    # ë‘˜ ë‹¤ ì œê³µí•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ CSV_PATH ("hoka_posts.csv")ì™€ ê²€ìƒ‰ í‚¤ì›Œë“œ "hoka"ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    csv_name = sys.argv[1] if len(sys.argv) > 1 else None
    search_keyword = sys.argv[2] if len(sys.argv) > 2 else KEYWORD
    update(csv_filename=csv_name, keyword=search_keyword)